import os
import requests
from bs4 import BeautifulSoup

def get_page_text(url):
    try:
        res = requests.get(url, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(res.text, "html.parser")
        # æœ¬æ–‡ã‚‰ã—ãæ®µè½ã‚’æŠ½å‡º
        paragraphs = [p.get_text() for p in soup.find_all("p")]
        return "\n".join(paragraphs[:10])  # æœ€åˆã®10æ®µè½ã‚’ã¾ã¨ã‚ã‚‹
    except Exception as e:
        print(f"âš ï¸ {url} ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ:", e)
        return ""

def get_latest_ai_news():
    api_key = os.environ.get("SEARCH_API_KEY")
    engine_id = os.environ.get("SEARCH_ENGINE_ID")
    query = "AI news site:news.google.com"

    url = f"https://www.googleapis.com/customsearch/v1?q={query}&key={api_key}&cx={engine_id}"
    res = requests.get(url)
    if res.status_code != 200:
        print("âŒ Search API ã‚¨ãƒ©ãƒ¼:", res.status_code)
        return []

    items = res.json().get("items", [])
    print(f"ğŸ” æ¤œç´¢çµæœä»¶æ•°: {len(items)}")

    results = []
    for item in items:
        article_url = item["link"]
        title = item["title"]
        full_text = get_page_text(article_url)

        if not full_text.strip():
            print(f"âš ï¸ {title} ã¯æœ¬æ–‡å–å¾—ã§ããšã‚¹ã‚­ãƒƒãƒ—")
            continue

        results.append({
            "title": title,
            "url": article_url,
            "content": full_text
        })

    return results
