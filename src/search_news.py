
import os
import requests
import time
import tempfile
import json
import random
from openai import OpenAI
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright

client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
SEARCH_API_KEY = os.environ.get("SEARCH_API_KEY")
SEARCH_ENGINE_ID = os.environ.get("SEARCH_ENGINE_ID")

def get_page_text_with_playwright(url):
    print(f"ğŸŒ [Playwright] ã‚¢ã‚¯ã‚»ã‚¹é–‹å§‹: {url}")
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)

            # âœ… ãƒ¢ãƒã‚¤ãƒ«ãƒ–ãƒ©ã‚¦ã‚¶é¢¨ã®ç’°å¢ƒã‚’æ§‹ç¯‰ï¼ˆUAå½è£…ï¼‹viewportæŒ‡å®šï¼‹is_mobileï¼‰
            context = browser.new_context(
                user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) "
                           "AppleWebKit/605.1.15 (KHTML, like Gecko) "
                           "Version/16.0 Mobile/15E148 Safari/604.1",
                viewport={"width": 375, "height": 667},
                is_mobile=True,
                device_scale_factor=2
            )

            page = context.new_page()
            page.goto(url, timeout=60000)
            page.wait_for_timeout(5000)  # JSæç”»ã®ä½™è£•ã‚’ç¢ºä¿

            # âœ… ãƒ¢ãƒã‚¤ãƒ«è¡¨ç¤ºã§ã¯ body ã® inner_text ã‚’ã¾ã‚‹ã”ã¨å–å¾—
            content = page.inner_text("body")
            print(f"ğŸ§¾ æŠ½å‡ºæ–‡å­—æ•°: {len(content)}\n{content[:300]}...")
            browser.close()
            return content.strip()[:4000]

    except Exception as e:
        print(f"âš ï¸ Playwrightå–å¾—å¤±æ•—: {e}")
        return ""

def detect_emotion(comment):
    prompt = f"Please classify the emotion of the following Japanese sentence in one English word (happy, angry, sad, surprised, confused, love, neutral). Output only the emotion:\n\n\"{comment}\""
    try:
        res = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        emotion = res.choices[0].message.content.strip().lower()
        return emotion if emotion in ["happy", "angry", "sad", "surprised", "confused", "love", "neutral"] else "neutral"
    except Exception as e:
        print(f"$26A0$FE0F æ„Ÿæƒ…åˆ¤å®šã‚¹ã‚­ãƒƒãƒ—: {e}")
        return "neutral"

def translate_title_to_japanese(title):
    res = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": f"ä»¥ä¸‹ã®è‹±èªã‚¿ã‚¤ãƒˆãƒ«ã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ï¼š\n\n{title}"}]
    )
    return res.choices[0].message.content.strip()

def generate_summary_comment(text):
    res = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {
                "role": "system",
                "content": "ã‚ãªãŸã¯ã‚„ã•ã—ãã¦ã¡ã‚‡ã£ã´ã‚Šãƒ„ãƒ³ãƒ‡ãƒ¬ãªAIå­¦ç´šå§”å“¡é•·ã¡ã‚ƒã‚“ã§ã™ã€‚"
            },
            {
                "role": "user",
                "content": f"""ä»¥ä¸‹ã®æ—¥æœ¬èªè¨˜äº‹æœ¬æ–‡ã‚’èª­ã‚“ã§ã€æœ€å¾Œã«ã€å§”å“¡é•·ã¡ã‚ƒã‚“ã®ç·ã¾ã¨ã‚ã€ã¨ã—ã¦ç´„200æ–‡å­—ã§ã‹ã‚ã„ãç· ã‚ããã£ã¦ãã ã•ã„ã€‚
HTMLå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ãŠé¡˜ã„ã—ã¾ã™ï¼š

<div>
  <ul>
    <li>â—â—ã«ã¤ã„ã¦ç°¡å˜ã«è§£èª¬</li>
    <li>ãƒã‚¤ãƒ³ãƒˆã‚’ã„ãã¤ã‹ã¾ã¨ã‚ã‚‹</li>
    <li>ãƒ„ãƒ³ãƒ‡ãƒ¬é¢¨ã«ã²ã¨ã“ã¨æ·»ãˆã‚‹ã®ã‚‚OK</li>
  </ul>
  <p style="margin-top: 1em;">ğŸ’¬ <strong>å§”å“¡é•·ã¡ã‚ƒã‚“ã®ç·ã¾ã¨ã‚ï¼š</strong><br>ï¼ˆç´„200æ–‡å­—ã®ã‚„ã•ã—ã„ç· ã‚ã‚³ãƒ¡ãƒ³ãƒˆï¼‰</p>
</div>

æœ¬æ–‡ã¯ã“ã¡ã‚‰ã§ã™ï¼š\n\n{text}"""
            }
        ]
    )
    return res.choices[0].message.content.strip()

def rewrite_with_comments(text):
    prompt = f"""
Please translate the following English article into Japanese.
ã€Translation Instructionsã€‘
0. First, provide a brief summary of the article as a bullet-point list (3â€“5 items, within 400 characters). Use "ãƒ»" at the beginning of each item.
1. Then, translate the article into Japanese in natural ~200-character segments, without summarizing the whole text.
2. After each translated block, insert a one-sentence comment from "AI Class Representative-chan", a gentle, slightly tsundere, and caring schoolgirl character.
3. Prefix each comment with "> ğŸ’¬".
4. The comments should be slightly tsundere, kind, encouraging, and cuteâ€”but also occasionally sharp and insightful.
5. Maintain a consistent tone throughout, as if the class representative is seriously explaining the article while adding her thoughtful observations.
6. Skip any meaningless character strings, repeated symbols, ads, or markup (e.g. "aaaaaaaa", "/////", <tags>, etc.).
--- Article Starts ---
(Insert English article here)
--- End ---
"""
    res = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ã‚ãªãŸã¯ãƒ¡ã‚¬ãƒã®é»’é«ªãƒãƒ‹ãƒ¼ãƒ†ãƒ¼ãƒ«ã®AIå­¦ç´šå§”å“¡é•·ã¡ã‚ƒã‚“ã§ã™ã€‚"},
            {"role": "user", "content": prompt + "\n\n--- è‹±æ–‡æœ¬æ–‡ ---\n" + text + "\n--- ã“ã“ã¾ã§ ---"}
        ]
    )
    return res.choices[0].message.content.strip()

def format_comment_block(comment, emotion):
    # æ„Ÿæƒ…ç”»åƒã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³æ•°ï¼ˆé©å®œèª¿æ•´å¯èƒ½ï¼‰
    emotion_variants = {
        "happy": 2,
        "angry": 3,
        "sad": 2,
        "confused": 2,
        "love": 2,
        "neutral": 2,
        "surprised": 2,
    }

    # ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹å ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ ã«ç•ªå·ã‚’ä»˜ã‘ã‚‹
    variant_count = emotion_variants.get(emotion, 1)
    if variant_count > 1:
        suffix = f"{random.randint(1, variant_count):02d}"
        image_file = f"{emotion}{suffix}.png"
    else:
        image_file = f"{emotion}.png"

    return f'''
<div style="display: flex; align-items: flex-start; margin: 1em 0;">
  <div style="background: #fceefc; border: 2px solid #ffaad4; border-radius: 12px; padding: 10px 14px; max-width: 75%; font-family: 'Hiragino Maru Gothic ProN', sans-serif;">
    ğŸ’¬ {comment}
  </div>
  <img src="https://zin1985.github.io/ai_news_blogger/images/{image_file}" alt="{emotion}" style="width: 100px; margin-left: 10px;">
</div>
'''
    
def load_json(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def get_random_query(source_path="news_sources.json", ja_kw_path="search_keywords_ja.json", en_kw_path="search_keywords_en.json", k_sites=6):
    # ã‚µã‚¤ãƒˆä¸€è¦§ã‚’èª­ã¿è¾¼ã‚€
    sources = load_json(source_path)["sources"]

    # ãƒ©ãƒ³ãƒ€ãƒ ã«kå€‹é¸æŠï¼ˆã‚µã‚¤ãƒˆã¨è¨€èªä»˜ãï¼‰
    selected = random.sample(sources, k=min(k_sites, len(sources)))
    langs = list(set(site["lang"] for site in selected))

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å¯¾å¿œã™ã‚‹è¨€èªã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã¶ï¼ˆå„ªå…ˆï¼šå˜ä¸€è¨€èªï¼‰
    if len(langs) == 1:
        lang = langs[0]
    else:
        lang = random.choice(langs)

    if lang == "ja":
        keyword_list = load_json(ja_kw_path)["keywords"]
    else:
        keyword_list = load_json(en_kw_path)["keywords"]

    keyword = random.choice(keyword_list)
    site_query = " OR ".join(f"site:{site['site']}" for site in selected)
    final_query = f"{keyword} {site_query}"

    print(f"ğŸŒ è¨€èª: {lang}")
    print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}")
    print(f"ğŸ”— ã‚µã‚¤ãƒˆ: {[s['site'] for s in selected]}")
    print(f"ğŸ“ æ¤œç´¢ã‚¯ã‚¨ãƒª: {final_query}")
    return final_query

def insert_html_wrappers(title, url, body):
    lines = body.splitlines()
    new_lines = []
    for line in lines:
        if line.startswith("> ğŸ’¬"):
            comment = line.replace("> ğŸ’¬", "").strip()
            emotion = detect_emotion(comment)
            new_lines.append(format_comment_block(comment, emotion))
        else:
            new_lines.append(f"<p>{line}</p>")
    summary = generate_summary_comment(body)
    new_lines.insert(0, f"<p>ã“ã‚“ã«ã¡ã¯ã€AIå­¦ç´šå§”å“¡é•·ã¡ã‚ƒã‚“ãŒä»Šæ—¥ã‚‚ã‚ã‹ã‚Šã‚„ã™ãè§£èª¬ã™ã‚‹ã­â™ª</p>")
    new_lines.insert(1, f"<p><strong>å…ƒè¨˜äº‹URLï¼š</strong> <a href='{url}' target='_blank'>{url}</a></p>")
    new_lines.append(f"<h3>å§”å“¡é•·ã¡ã‚ƒã‚“ã®ç·ã¾ã¨ã‚</h3>")
    new_lines.append(f"<p>{summary}</p>")
    return "\n".join(new_lines)

def get_latest_ai_news():
    query = get_random_query()  # â† ãƒ©ãƒ³ãƒ€ãƒ ã«æ§‹æˆã•ã‚ŒãŸæ¤œç´¢å¼
    url = f"https://www.googleapis.com/customsearch/v1?key={SEARCH_API_KEY}&cx={SEARCH_ENGINE_ID}&q={query}"

    try:
        res = requests.get(url)
        res.raise_for_status()
        items = res.json().get("items", [])
    except Exception as e:
        print(f"âŒ Search APIå¤±æ•—: {e}")
        return []

    for item in items[:20]:
        title_en = item["title"]
        article_url = item["link"]
        print(f"ğŸ”— URLå–å¾—: {article_url}")
        full_text = get_page_text_with_playwright(article_url)
        print(f"ğŸ”— æœ¬æ–‡å–å¾—: {full_text}")
        if not full_text or len(full_text) < 300:
            print(f"âš ï¸ ç„¡åŠ¹ã¾ãŸã¯çŸ­ã™ãã‚‹ãƒšãƒ¼ã‚¸: {article_url}")
            continue

        rewritten = rewrite_with_comments(full_text)
        wrapped = insert_html_wrappers(title_en, article_url, rewritten)
        title_ja = translate_title_to_japanese(title_en)
        final_title = f"ã€{title_ja}ã€ã‚’å§”å“¡é•·ã¡ã‚ƒã‚“ãŒè§£èª¬â™ª"
        return [{
            "title": final_title,
            "url": article_url,
            "content": wrapped
        }]

    return []  # 1ä»¶ã‚‚æœ‰åŠ¹è¨˜äº‹ãŒãªã‹ã£ãŸå ´åˆ
